{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsClassification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOWlpYcnMH2v+Uj5XKjbW4q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1KI_F7zfD2U"
      },
      "outputs": [],
      "source": [
        "# check news data \n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters  # reuter news data from keras dataset- tokenization, encoding completed): news = 11,258  category = 46\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data split(8:2) \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "print('news as training data : {}'.format(len(X_train)))\n",
        "print('news as test data : {}'.format(len(X_test)))\n",
        "num_classes = len(set(y_train))\n",
        "print('news_classes : {}'.format(num_classes))"
      ],
      "metadata": {
        "id": "FLph0RNPKTv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('first news in traing data :',X_train[0])\n",
        "print('news label of the first news :',y_train[0])"
      ],
      "metadata": {
        "id": "RM-GaAjSK4F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('the longest length of the news samples :{}'.format(max(len(sample) for sample in X_train)))\n",
        "print('the average length of the news samples :{}'.format(sum(map(len, X_train))/len(X_train)))\n",
        "\n",
        "plt.hist([len(sample) for sample in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_l8KIer8LSVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of news labels\n",
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(12,5)\n",
        "sns.countplot(y_train)"
      ],
      "metadata": {
        "id": "SRscdJ7MLqVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(\"frequency of each label:\")\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "id": "PKF4lEn7L0DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word index\n",
        "word_to_index = reuters.get_word_index()\n",
        "#print(word_to_index)"
      ],
      "metadata": {
        "id": "dmTsV5FmMJDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value+3] = key  # index to word +3 (0: padding, 1: sos, 2: OOV)\n",
        "\n",
        "print('first word in the freq list  : {}'.format(index_to_word[4]))\n",
        "print('128th word in the freq list  : {}'.format(index_to_word[131]))"
      ],
      "metadata": {
        "id": "j9hUGMg2MKCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# include 3 tokens (pad, sos, unk) into index_to_word (following Keras rules for reuter news dataset)\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index] = token\n",
        "\n",
        "print(' '.join([index_to_word[index] for index in X_train[0]]))  "
      ],
      "metadata": {
        "id": "YbjdLzLTMrDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# news classification using LSTM\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "vocab_size = 1000\n",
        "max_len = 100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "03LpRRTLN46p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(num_classes, activation='softmax'))   # multi-class classification \n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)  # val_loss increases 4 times continuously, stop training to avoid overfitting\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # store the model only val_acc is better than before\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "JfHQkB6OOc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n test accuracy: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "YvfY3qtXR-UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szS98OnDSRiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}